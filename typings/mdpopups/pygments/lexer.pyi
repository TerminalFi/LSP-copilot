from .util import Future
from _typeshed import Incomplete
from collections.abc import Generator

__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer', 'LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this', 'default', 'words']

class LexerMeta(type):
    def __new__(cls, name, bases, d): ...

class Lexer:
    name: Incomplete
    aliases: Incomplete
    filenames: Incomplete
    alias_filenames: Incomplete
    mimetypes: Incomplete
    priority: int
    options: Incomplete
    stripnl: Incomplete
    stripall: Incomplete
    ensurenl: Incomplete
    tabsize: Incomplete
    encoding: Incomplete
    filters: Incomplete
    def __init__(self, **options) -> None: ...
    def add_filter(self, filter_, **options) -> None: ...
    def analyse_text(text) -> None: ...
    def get_tokens(self, text, unfiltered: bool = False): ...
    def get_tokens_unprocessed(self, text) -> None: ...

class DelegatingLexer(Lexer):
    root_lexer: Incomplete
    language_lexer: Incomplete
    needle: Incomplete
    def __init__(self, _root_lexer, _language_lexer, _needle=..., **options) -> None: ...
    def get_tokens_unprocessed(self, text): ...

class include(str): ...
class _inherit: ...

inherit: Incomplete

class combined(tuple):
    def __new__(cls, *args): ...
    def __init__(self, *args) -> None: ...

class _PseudoMatch:
    def __init__(self, start, text) -> None: ...
    def start(self, arg: Incomplete | None = None): ...
    def end(self, arg: Incomplete | None = None): ...
    def group(self, arg: Incomplete | None = None): ...
    def groups(self): ...
    def groupdict(self): ...

def bygroups(*args): ...

class _This: ...

this: Incomplete

def using(_other, **kwargs): ...

class default:
    state: Incomplete
    def __init__(self, state) -> None: ...

class words(Future):
    words: Incomplete
    prefix: Incomplete
    suffix: Incomplete
    def __init__(self, words, prefix: str = '', suffix: str = '') -> None: ...
    def get(self): ...

class RegexLexerMeta(LexerMeta):
    def process_tokendef(cls, name, tokendefs: Incomplete | None = None): ...
    def get_tokendefs(cls): ...
    def __call__(cls, *args, **kwds): ...

class RegexLexer(Lexer):
    flags: Incomplete
    tokens: Incomplete
    def get_tokens_unprocessed(self, text, stack=('root',)) -> Generator[Incomplete]: ...

class LexerContext:
    text: Incomplete
    pos: Incomplete
    end: Incomplete
    stack: Incomplete
    def __init__(self, text, pos, stack: Incomplete | None = None, end: Incomplete | None = None) -> None: ...

class ExtendedRegexLexer(RegexLexer):
    def get_tokens_unprocessed(self, text: Incomplete | None = None, context: Incomplete | None = None) -> Generator[Incomplete]: ...

class ProfilingRegexLexerMeta(RegexLexerMeta): ...

class ProfilingRegexLexer(RegexLexer):
    def get_tokens_unprocessed(self, text, stack=('root',)) -> Generator[Incomplete, None, Incomplete]: ...
